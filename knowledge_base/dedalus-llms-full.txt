

# Hello World
Source: https://docs.dedaluslabs.ai/examples/01-hello-world

Basic chat completion with Dedalus

This example demonstrates the most basic usage of the Dedalus SDK - making a simple chat completion call.

<CodeGroup>
  ```python Python theme={null}
  import asyncio
  from dedalus_labs import AsyncDedalus, DedalusRunner
  from dotenv import load_dotenv
  from dedalus_labs.utils.streaming import stream_async

  load_dotenv()

  async def main():
      client = AsyncDedalus()
      runner = DedalusRunner(client)

      response = await runner.run(
          input="What was the score of the 2025 Wimbledon final?",
          model="openai/gpt-5-mini"
      )

      print(response.final_output)

  if __name__ == "__main__":
      asyncio.run(main())
  ```

  ```typescript TypeScript theme={null}
  Coming *Very* Soon
  ```
</CodeGroup>


# Basic Tools
Source: https://docs.dedaluslabs.ai/examples/02-basic-tools

Clean tool execution with the new Runner

This example demonstrates basic tool execution using the Dedalus Runner with simple mathematical tools.

<Tip>
  GPT 5 or 4.1 (`openai/gpt-5` or `openai/gpt-4.1`) are strong tool-calling models. In general, older models may struggle with tool calling.
</Tip>

<CodeGroup>
  ```python Python theme={null}
  import asyncio
  from dedalus_labs import AsyncDedalus, DedalusRunner
  from dotenv import load_dotenv
  from dedalus_labs.utils.streaming import stream_async

  load_dotenv()

  def add(a: int, b: int) -> int:
      """Add two numbers."""
      return a + b

  def multiply(a: int, b: int) -> int:
      """Multiply two numbers."""
      return a * b

  async def main():
      client = AsyncDedalus()
      runner = DedalusRunner(client)

      result = await runner.run(
          input="Calculate (15 + 27) * 2", 
          model="openai/gpt-5", 
          tools=[add, multiply]
      )

      print(f"Result: {result.final_output}")

  if __name__ == "__main__":
      asyncio.run(main())
  ```

  ```typescript TypeScript theme={null}
  Coming *Very* Soon
  ```
</CodeGroup>


# Streaming
Source: https://docs.dedaluslabs.ai/examples/03-streaming

Streaming responses with Agent system

This example demonstrates streaming agent output using the built-in streaming support with the Agent system.

<CodeGroup>
  ```python Async Streaming theme={null}
  import asyncio
  from dedalus_labs import AsyncDedalus, DedalusRunner
  from dotenv import load_dotenv
  from dedalus_labs.utils.streaming import stream_async

  load_dotenv()

  async def main():
      client = AsyncDedalus()
      runner = DedalusRunner(client)

      result = runner.run(
          input="What do you think of Mulligan?",
          model="openai/gpt-5-mini",
          stream=True
      )

      # use stream parameter and stream_async function to stream output
      await stream_async(result)

  if __name__ == "__main__":
      asyncio.run(main())
  ```

  ```python Sync Streaming theme={null}
  from dedalus_labs import Dedalus, DedalusRunner
  from dotenv import load_dotenv
  from dedalus_labs.utils.streaming import stream_sync

  load_dotenv()

  def main():
      client = Dedalus()
      runner = DedalusRunner(client)

      result = runner.run(
          input="What do you think of Mulligan?",
          model="openai/gpt-5-mini",
          stream=True
      )

      # use stream parameter and stream_sync function to stream output
      stream_sync(result)

  if __name__ == "__main__":
      main()
  ```

  ```typescript TypeScript theme={null}
  Coming *Very* Soon
  ```
</CodeGroup>


# MCP Integration
Source: https://docs.dedaluslabs.ai/examples/04-mcp-integration

Basic remote MCP server usage with the Dedalus SDK

This example demonstrates basic remote MCP (Model Context Protocol) server usage with the Dedalus SDK for connecting to external tools and services.

<CodeGroup>
  ```python Python theme={null}
  import asyncio
  from dedalus_labs import AsyncDedalus, DedalusRunner
  from dotenv import load_dotenv
  from dedalus_labs.utils.streaming import stream_async

  load_dotenv()

  async def main():
      client = AsyncDedalus()
      runner = DedalusRunner(client)

      result = await runner.run(
          input="Who won Wimbledon 2025?",
          model="openai/gpt-5-mini",
          mcp_servers=["windsor/brave-search-mcp"],
          stream=False
      )

      print(result.final_output)

  if __name__ == "__main__":
      asyncio.run(main())
  ```

  ```typescript TypeScript theme={null}
  Coming *Very* Soon
  ```
</CodeGroup>


# Model Handoffs
Source: https://docs.dedaluslabs.ai/examples/06-handoffs

Multi-model routing where the agent intelligently selects the best model based on task complexity

This example demonstrates multi-model routing where the agent intelligently selects the best model based on task complexity, with model attributes for optimization.

<Tip>
  Claude (`anthropic/claude-sonnet-4-20250514`) is great at writing and creative tasks. Experiment with different models for different use-cases!
</Tip>

<CodeGroup>
  ```python Python theme={null}
  import os
  from dedalus_labs import AsyncDedalus, DedalusRunner
  from dotenv import load_dotenv
  from dedalus_labs.utils.streaming import stream_async
  import asyncio

  load_dotenv()

  async def main():
      client = AsyncDedalus()
      runner = DedalusRunner(client)

      result = await runner.run(
          input="Find the year GPT-5 released, and handoff to Claude to write a haiku about Elon Musk. Output this haiku. Use your tools.",
          model=["openai/gpt-5", "claude-sonnet-4-20250514"],
          mcp_servers=["windsor/brave-search-mcp"],
          stream=False
      )

      print(result.final_output)

  if __name__ == "__main__":
      asyncio.run(main())
  ```

  ```typescript TypeScript theme={null}
  Coming *Very* Soon
  ```
</CodeGroup>


# Tool Chaining
Source: https://docs.dedaluslabs.ai/examples/07-tool-chaining

Advanced tool chaining workflow with async execution

This example demonstrates advanced tool chaining where multiple tools are executed in sequence, with the Runner handling complex multi-step workflows automatically.

<Tip>
  GPT 5 or 4.1 (`openai/gpt-5` or `openai/gpt-4.1`) are strong tool-calling models. In general, older models may struggle with tool calling.
</Tip>

<CodeGroup>
  ```python Python theme={null}
  import asyncio
  from dedalus_labs import AsyncDedalus, DedalusRunner
  from dotenv import load_dotenv
  from dedalus_labs.utils.streaming import stream_async

  load_dotenv()

  def celsius_to_fahrenheit(celsius: float) -> float:
      """Convert temperature from Celsius to Fahrenheit."""
      return (celsius * 9/5) + 32

  def get_clothing_recommendation(temp_f: float) -> str:
      """Recommend clothing based on temperature in Fahrenheit."""
      if temp_f < 32:
          return "Heavy winter coat, gloves, hat, and warm boots"
      elif temp_f < 50:
          return "Warm jacket or coat, long pants, closed shoes"
      elif temp_f < 65:
          return "Light jacket or sweater, long pants"
      elif temp_f < 80:
          return "T-shirt or light shirt, comfortable pants or shorts"
      else:
          return "Lightweight clothing, shorts, sandals, and sun protection"

  def plan_activity(temp_f: float, clothing: str) -> str:
      """Suggest outdoor activities based on temperature and clothing."""
      if temp_f < 32:
          return f"Great weather for skiing, ice skating, or cozy indoor activities. Dress in: {clothing}"
      elif temp_f < 50:
          return f"Perfect for hiking, jogging, or outdoor photography. Dress in: {clothing}"
      elif temp_f < 80:
          return f"Ideal for picnics, outdoor sports, or walking in the park. Dress in: {clothing}"
      else:
          return f"Excellent for swimming, beach activities, or water sports. Dress in: {clothing}"

  async def main():
      client = AsyncDedalus()
      runner = DedalusRunner(client)

      result = await runner.run(
          input="It's 22 degrees Celsius today in Paris. Convert this to Fahrenheit, recommend what I should wear, suggest outdoor activities, and search for current weather conditions in Paris to confirm.",
          model=["openai/gpt-5"],
          tools=[celsius_to_fahrenheit, get_clothing_recommendation, plan_activity],
          mcp_servers=["joerup/open-meteo-mcp", "windsor/brave-search-mcp"],
          stream=False
      )

      print(result.final_output)

  if __name__ == "__main__":
      asyncio.run(main())
  ```

  ```typescript TypeScript theme={null}
  Coming *Very* Soon
  ```
</CodeGroup>


# Concert Planner
Source: https://docs.dedaluslabs.ai/examples/use-case/concert-planner

Create a concert planner agent using the Ticketmaster MCP to search for concerts and venue information.

<CodeGroup>
  ```python Python theme={null}
  import asyncio
  from dedalus_labs import AsyncDedalus, DedalusRunner
  from dotenv import load_dotenv
  from dedalus_labs.utils.streaming import stream_async

  load_dotenv()

  async def main():
      client = AsyncDedalus()
      runner = DedalusRunner(client)

      result = await runner.run(
          input="""I want to see Taylor Swift perform in New York City. 
          Can you help me find upcoming concerts, get details about the venue, 
          and provide information about ticket prices? I'm particularly interested 
          in accessibility information and seating options.""",
          model="openai/gpt-4.1",
          mcp_servers=["windsor/ticketmaster-mcp"]
      )

      print(f"Concert Planning Results:\n{result.final_output}")

  if __name__ == "__main__":
      asyncio.run(main())
  ```

  ```typescript TypeScript theme={null}
  Coming *Very* Soon
  ```
</CodeGroup>

<Tip>
  This example uses the Ticketmaster MCP (`windsor/ticketmaster-mcp`).

  Try it out in your projects!
</Tip>


# Data Analyst
Source: https://docs.dedaluslabs.ai/examples/use-case/data-analyst

Create a data analyst agent that can search for real-time data, write and execute Python code to analyze it, and generate insights.

<CodeGroup>
  ```python Python theme={null}
  import asyncio
  from dedalus_labs import AsyncDedalus, DedalusRunner
  from dedalus_labs.utils.streaming import stream_async
  from dotenv import load_dotenv

  load_dotenv()

  def execute_python_code(code: str) -> str:
      """
      Execute Python code and return the result.
      Safely executes code in a controlled namespace.
      """
      try:
          namespace = {}
          exec(code, {"__builtins__": __builtins__}, namespace)

          if 'result' in namespace:
              return str(namespace['result'])

          results = {k: v for k, v in namespace.items() if not k.startswith('_')}
          return str(results) if results else "Code executed successfully"
      except Exception as e:
          return f"Error executing code: {str(e)}"

  async def main():
      client = AsyncDedalus()
      runner = DedalusRunner(client)

      result = runner.run(
          input="""Research the current stock price of Tesla (TSLA) and Apple (AAPL).
          Then write and execute Python code to:
          1. Compare their current prices
          2. Calculate the percentage difference
          3. Determine which stock has grown more in the past year based on the data you find
          4. Provide investment insights based on your analysis

          Use web search to get the latest stock information.""",
          model="openai/gpt-5",
          tools=[execute_python_code],
          mcp_servers=["windsor/brave-search-mcp"],
          stream=True
      )

      await stream_async(result)

  if __name__ == "__main__":
      asyncio.run(main())
  ```

  ```typescript TypeScript theme={null}
  Coming *Very* Soon
  ```
</CodeGroup>

<Tip>
  This data analyst example combines real-time web search with code execution capabilities:

  * **Brave Search MCP** (`windsor/brave-search-mcp`): Fetches real-time data from the web
  * **execute\_python\_code** tool: Allows the agent to write and run Python code for analysis

  The agent can search for current information, extract relevant data, then dynamically write code to analyze it and generate insights.

  **Note**: In production environments, consider using sandboxed code execution for security.
</Tip>


# Travel Agent
Source: https://docs.dedaluslabs.ai/examples/use-case/travel-agent

Creating a travel planning agent that can search for flights, hotels, and provide travel recommendations.

<CodeGroup>
  ```python Python theme={null}
  import asyncio
  from dedalus_labs import AsyncDedalus, DedalusRunner
  from dotenv import load_dotenv
  from dedalus_labs.utils.streaming import stream_async

  load_dotenv()

  async def main():
      client = AsyncDedalus()
      runner = DedalusRunner(client)

      result = await runner.run(
          input="""I'm planning a trip to Paris, France from New York City 
          for 5 days in October. Can you help me find:
          1. Flight options and prices
          2. Hotel recommendations in central Paris
          3. Weather forecast for my travel dates
          4. Popular attractions and restaurants
          5. Currency exchange rates and travel tips
          
          My budget is around $3000 total and I prefer mid-range accommodations.""",
          model="openai/gpt-4.1",
          mcp_servers=[
              "joerup/exa-mcp",        # For semantic travel research
              "windsor/brave-search-mcp", # For travel information search
              "joerup/open-meteo-mcp"   # For weather at destination
          ]
      )

      print(f"Travel Planning Results:\n{result.final_output}")

  if __name__ == "__main__":
      asyncio.run(main())
  ```

  ```typescript TypeScript theme={null}
  Coming *Very* Soon
  ```
</CodeGroup>

<Tip>
  This travel agent example uses multiple MCP servers:

  * **Exa MCP** (`joerup/exa-mcp`): For semantic search of travel content and recommendations
  * **Brave Search MCP** (`windsor/brave-search-mcp`): For finding current travel information, reviews, and booking options
  * **Open Meteo MCP** (`joerup/open-meteo-mcp`): For weather forecasts at your destination

  Try these servers out in your projects!
</Tip>


# Weather Forecaster
Source: https://docs.dedaluslabs.ai/examples/use-case/weather-forecaster

Create a weather forecasting agent using the Open Meteo MCP to provide detailed weather information and forecasts.

<CodeGroup>
  ```python Python theme={null}
  import asyncio
  from dedalus_labs import AsyncDedalus, DedalusRunner
  from dotenv import load_dotenv
  from dedalus_labs.utils.streaming import stream_async

  load_dotenv()

  async def main():
      client = AsyncDedalus()
      runner = DedalusRunner(client)

      result = await runner.run(
          input="""I'm planning a outdoor wedding in San Francisco next weekend. 
          Can you provide:
          1. Current weather conditions
          2. 7-day forecast with hourly details
          3. Probability of precipitation
          4. Temperature highs and lows
          5. Wind conditions and UV index
          6. Recommendations for outdoor event planning based on the forecast""",
          model="openai/gpt-4.1",
          mcp_servers=["joerup/open-meteo-mcp"]
      )

      print(f"Weather Forecast Results:\n{result.final_output}")

  if __name__ == "__main__":
      asyncio.run(main())
  ```

  ```typescript TypeScript theme={null}
  Coming *Very* Soon
  ```
</CodeGroup>

<Tip>
  This example uses the Open Meteo MCP (`joerup/open-meteo-mcp`) which provides:

  * Current weather conditions
  * Multi-day weather forecasts
  * Hourly weather data
  * Historical weather information
  * Weather alerts and warnings

  Try it out in your projects!
</Tip>


# Web Search Agent
Source: https://docs.dedaluslabs.ai/examples/use-case/web-search-agent

Create a web search agent using multiple search MCPs to find and analyze information from the web.

<CodeGroup>
  ```python Python theme={null}
  import asyncio
  from dedalus_labs import AsyncDedalus, DedalusRunner
  from dotenv import load_dotenv
  from dedalus_labs.utils.streaming import stream_async

  load_dotenv()

  async def main():
      client = AsyncDedalus()
      runner = DedalusRunner(client)

      result = await runner.run(
          input="""I need to research the latest developments in AI agents for 2024. 
          Please help me:
          1. Find recent news articles about AI agent breakthroughs
          2. Search for academic papers on multi-agent systems
          3. Look up startup companies working on AI agents
          4. Find GitHub repositories with popular agent frameworks
          5. Summarize the key trends and provide relevant links
          
          Focus on developments from the past 6 months.""",
          model="openai/gpt-4.1",
          mcp_servers=[
              "joerup/exa-mcp",        # Semantic search engine
              "windsor/brave-search-mcp"  # Privacy-focused web search
          ]
      )

      print(f"Web Search Results:\n{result.final_output}")

  if __name__ == "__main__":
      asyncio.run(main())
  ```

  ```typescript TypeScript theme={null}
  Coming *Very* Soon
  ```
</CodeGroup>

<Tip>
  This example uses multiple search MCP servers:

  * **Exa MCP** (`joerup/exa-mcp`)
  * **Brave Search MCP** (`windsor/brave-search-mcp`)

  Try these servers out in your projects!
</Tip>


# FAQ
Source: https://docs.dedaluslabs.ai/faq

Frequently Asked Questions

<AccordionGroup>
  <Accordion icon="question" title="Why use Dedalus?" defaultOpen>
    * We make it easy to build complex AI agents with just 5 (or so) lines of code.
    * Agents built with our SDK can connect to any MCP server on our marketplace, switch between any model provider, and even execute locally-defined tools.
    * Don’t yet see an MCP you want to use on our marketplace? Upload any MCP server and we’ll host it for free.
  </Accordion>

  <Accordion icon="key" title="How do I get an API key?" defaultOpen>
    Log into your [dashboard](https://dedaluslabs.ai) and navigate to the "API Keys" section.
  </Accordion>

  <Accordion icon="key" title="Can I bring my own API key?" defaultOpen>
    Yes! However, you don't need to. With a `DEDALUS_API_KEY` in your environment, we take care of routing to any provider or model for you, including handoffs between models from different providers. For an example, see our [handoffs](https://docs.dedaluslabs.ai/examples/06-handoffs) page.
  </Accordion>

  <Accordion icon="code" title="What languages do you support?" defaultOpen>
    Our SDK is currently available for Python and TypeScript (beta), with plans for Go in the near future. We accept MCP servers written Python and TypeScript. For best practices in writing MCP servers see our [server guidelines](https://docs.dedaluslabs.ai/server-guidelines).
  </Accordion>

  <Accordion icon="lock" title="Is authentication supported?" defaultOpen>
    Not yet, but it's coming soon! Until authentication is supported, please ensure your servers are stateless and do not require auth.
  </Accordion>

  <Accordion icon="envelope" title="How do I send feedback?" defaultOpen>
    Send us an email at [support@dedaluslabs.ai](mailto:support@dedaluslabs.ai) or send a message in our [Discord](https://discord.gg/K3SjuFXZJw).
  </Accordion>
</AccordionGroup>


# Quickstart
Source: https://docs.dedaluslabs.ai/index

Get started with the Dedalus SDK.

# Setup and Integration

<AccordionGroup defaultOpen>
  <Accordion icon="key" title="Get Your API Key" defaultOpen>
    To use Dedalus Labs, you'll need an API key. To get one:

    1. Create an account at [dedaluslabs.ai](https://dedaluslabs.ai).
    2. Navigate to your dashboard.
    3. Generate a new API key in the settings section.
    4. Add your API key to your environment as `DEDALUS_API_KEY`. Keep your API key secure and never share it publicly.
  </Accordion>

  <Accordion icon="code" title="Install Our SDK" defaultOpen>
    We provide SDKs for multiple programming languages to make integration seamless:

    <CodeGroup>
      ```bash Python theme={null}
      pip install dedalus-labs
      ```

      ```bash TypeScript theme={null}
      npm install dedalus-labs
      ```
    </CodeGroup>
  </Accordion>

  <Accordion icon="hand-wave" title="Hello World" defaultOpen>
    This example demonstrates the most basic usage of the Dedalus SDK - making a simple chat completion call.

    <CodeGroup>
      ```python Python theme={null}
      import asyncio
      from dedalus_labs import AsyncDedalus, DedalusRunner
      from dotenv import load_dotenv
      from dedalus_labs.utils.streaming import stream_async

      load_dotenv()

      async def main():
          client = AsyncDedalus()
          runner = DedalusRunner(client)

          response = await runner.run(
              input="What was the score of the 2025 Wimbledon final?",
              model="openai/gpt-5-mini"
          )

          print(response.final_output)

      if __name__ == "__main__":
          asyncio.run(main())
      ```

      ```typescript TypeScript theme={null}
      Coming *Very* Soon
      ```
    </CodeGroup>
  </Accordion>

  <Accordion icon="hammer" title="Basic Tools" defaultOpen>
    This example demonstrates basic tool execution using the Dedalus Runner with simple mathematical tools.

    <Tip>
      GPT 5 or 4.1 (`openai/gpt-5` or `openai/gpt-4.1`) are strong tool-calling models. In general, older models may struggle with tool calling.
    </Tip>

    <CodeGroup>
      ```python Python theme={null}
      import asyncio
      from dedalus_labs import AsyncDedalus, DedalusRunner
      from dotenv import load_dotenv
      from dedalus_labs.utils.streaming import stream_async

      load_dotenv()

      def add(a: int, b: int) -> int:
          """Add two numbers."""
          return a + b

      def multiply(a: int, b: int) -> int:
          """Multiply two numbers."""
          return a * b

      async def main():
          client = AsyncDedalus()
          runner = DedalusRunner(client)

          result = await runner.run(
              input="Calculate (15 + 27) * 2",
              model="openai/gpt-5",
              tools=[add, multiply]
          )

          print(f"Result: {result.final_output}")

      if __name__ == "__main__":
          asyncio.run(main())
      ```

      ```typescript TypeScript theme={null}
      Coming *Very* Soon
      ```
    </CodeGroup>
  </Accordion>
</AccordionGroup>

<CardGroup cols={4}>
  <Card title="Streaming" icon="water" href="examples/03-streaming" />

  <Card title="MCP Integration" icon="plug" href="examples/04-mcp-integration" />

  <Card title="Model Handoffs" icon="arrows-rotate" href="examples/06-handoffs" />

  <Card title="Tool Chaining" icon="link" href="examples/07-tool-chaining" />
</CardGroup>

# Next Steps

<Card title="View the Examples" icon="book-open" href="examples/use-case/travel-agent">
  See detailed examples for your use-case
</Card>

<Card title="Join Our Community" icon="discord" href="https://discord.com/invite/RuDhZKnq5R">
  Get help, suggest improvements, and connect with other developers.
</Card>


# Introduction
Source: https://docs.dedaluslabs.ai/introduction

Dedalus Labs is building the MCP gateway for next-gen AI applications by unifying the fragmented AI-agent ecosystem with a **single drop-in API endpoint**. 

<CardGroup cols={2}>
  <Card title="Quickstart" img="https://mintcdn.com/dedaluslabs/h04RAzy7Uc9PSpSd/images/card-design-1.png?fit=max&auto=format&n=h04RAzy7Uc9PSpSd&q=85&s=030d4cfe0974199ec62070046fa7c587" href="/index" data-og-width="2408" width="2408" data-og-height="2512" height="2512" data-path="images/card-design-1.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dedaluslabs/h04RAzy7Uc9PSpSd/images/card-design-1.png?w=280&fit=max&auto=format&n=h04RAzy7Uc9PSpSd&q=85&s=2601424e7cd8e9a13a02a6362b61895b 280w, https://mintcdn.com/dedaluslabs/h04RAzy7Uc9PSpSd/images/card-design-1.png?w=560&fit=max&auto=format&n=h04RAzy7Uc9PSpSd&q=85&s=918e1ab518e882df96a534dd206067ce 560w, https://mintcdn.com/dedaluslabs/h04RAzy7Uc9PSpSd/images/card-design-1.png?w=840&fit=max&auto=format&n=h04RAzy7Uc9PSpSd&q=85&s=65fd6654e049ec9668f46952d457607e 840w, https://mintcdn.com/dedaluslabs/h04RAzy7Uc9PSpSd/images/card-design-1.png?w=1100&fit=max&auto=format&n=h04RAzy7Uc9PSpSd&q=85&s=52ad009f3a1d2b0f6835fd2539deddfc 1100w, https://mintcdn.com/dedaluslabs/h04RAzy7Uc9PSpSd/images/card-design-1.png?w=1650&fit=max&auto=format&n=h04RAzy7Uc9PSpSd&q=85&s=6cc6c7d03147c8f79a98c9a0367d24f2 1650w, https://mintcdn.com/dedaluslabs/h04RAzy7Uc9PSpSd/images/card-design-1.png?w=2500&fit=max&auto=format&n=h04RAzy7Uc9PSpSd&q=85&s=ebed645f11a8ef0a63eb0ff295a51daf 2500w">
    Jump right in and start building agents with our SDK.
  </Card>

  <Card title="Examples" img="https://mintcdn.com/dedaluslabs/h04RAzy7Uc9PSpSd/images/card-design-wing.png?fit=max&auto=format&n=h04RAzy7Uc9PSpSd&q=85&s=0b96280d21a77d0078fb4df9666fb912" href="/examples/use-case/travel-agent" data-og-width="2408" width="2408" data-og-height="2512" height="2512" data-path="images/card-design-wing.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dedaluslabs/h04RAzy7Uc9PSpSd/images/card-design-wing.png?w=280&fit=max&auto=format&n=h04RAzy7Uc9PSpSd&q=85&s=821dfbbc850b54fc4b7b0b29d6823f15 280w, https://mintcdn.com/dedaluslabs/h04RAzy7Uc9PSpSd/images/card-design-wing.png?w=560&fit=max&auto=format&n=h04RAzy7Uc9PSpSd&q=85&s=c1d1b9669ddf8d171384a9dfa6e74d2f 560w, https://mintcdn.com/dedaluslabs/h04RAzy7Uc9PSpSd/images/card-design-wing.png?w=840&fit=max&auto=format&n=h04RAzy7Uc9PSpSd&q=85&s=01ab66a6c5bc4bc0fea07f211181e957 840w, https://mintcdn.com/dedaluslabs/h04RAzy7Uc9PSpSd/images/card-design-wing.png?w=1100&fit=max&auto=format&n=h04RAzy7Uc9PSpSd&q=85&s=e822e6d58ef78a51c476d4f0f4a439e0 1100w, https://mintcdn.com/dedaluslabs/h04RAzy7Uc9PSpSd/images/card-design-wing.png?w=1650&fit=max&auto=format&n=h04RAzy7Uc9PSpSd&q=85&s=73a9ecf07c53242542d0c5a12175f6a0 1650w, https://mintcdn.com/dedaluslabs/h04RAzy7Uc9PSpSd/images/card-design-wing.png?w=2500&fit=max&auto=format&n=h04RAzy7Uc9PSpSd&q=85&s=a816fd81baa63f08d03935cf65870819 2500w">
    Explore real-world examples and use cases to inspire your projects.
  </Card>
</CardGroup>

## Give Your Agents Wings with Dedalus

* We host and manage your MCP servers
* Users can select models from any vendor and combine various tools from our MCP marketplace
* Teams can transition from concept to functioning agents with tools in minutes


# Model Providers
Source: https://docs.dedaluslabs.ai/providers

Mix and match models from supported providers.

<CardGroup cols={3}>
  <Card title="OpenAI" icon="robot">
    `OPENAI_API_KEY`
  </Card>

  <Card title="Anthropic" icon="brain">
    `ANTHROPIC_API_KEY`
  </Card>

  <Card title="Google Gemini" icon="google">
    `GOOGLE_API_KEY`
  </Card>

  <Card title="Fireworks AI" icon="fire">
    `FIREWORKS_API_KEY`
  </Card>

  <Card title="xAI" icon="x">
    `XAI_API_KEY`
  </Card>

  <Card title="Perplexity" icon="circle-question">
    `PERPLEXITY_API_KEY`
  </Card>

  <Card title="DeepSeek" icon="magnifying-glass">
    `DEEPSEEK_API_KEY`
  </Card>

  <Card title="Groq" icon="bolt">
    `GROQ_API_KEY`
  </Card>

  <Card title="Cohere" icon="comments">
    `COHERE_API_KEY`
  </Card>

  <Card title="Together AI" icon="users">
    `TOGETHERAPI_KEY`
  </Card>

  <Card title="Cerebras" icon="microchip">
    `CEREBRAS_API_KEY`
  </Card>

  <Card title="Mistral" icon="wind">
    `MISTRAL_API_KEY`
  </Card>
</CardGroup>


# MCP Server Guidelines
Source: https://docs.dedaluslabs.ai/server-guidelines

Ensure your server works with the Dedalus platform.

<Warning>
  We're open-sourcing an MCP framework to help users easily build and deploy their MCP servers in October 2025.
  Until then, users are encouraged to structure their MCP servers according to the [templates](#key-details) below.
</Warning>

# Key Details

* **We support both TypeScript and Python servers:**
  * **TypeScript servers:** We look for an `index.ts` in `src/`. The simplest server is a repo with a `src/` folder with `index.ts` in it that starts the server. Use this [template](https://github.com/windsornguyen/brave-search-mcp) ([copy as markdown](https://gitingest.com/dedalus-labs/brave-search-mcp)).
  * **Python servers:** We look for a `main.py` in `src/`. The simplest server is a repo with a `src/` folder with `main.py` in it that starts the server. Use this [template](https://github.com/dedalus-labs/mcp-server-example-python) ([copy as markdown](https://gitingest.com/dedalus-labs/mcp-server-example-python)).
* Since servers will be remotely deployed, they must use the streamable HTTP transport method.

<Warning>
  Authentication is under rapid development but is not currently supported. Accordingly, your servers should be stateless and not require auth.
</Warning>

# Full MCP Server Architecture Guide

<Tip>
  Pro tip: Click the "Copy page" button to paste this page as context to your coding assistant to refactor your existing server to follow our recommended specification.
</Tip>

## Overview

This guide defines the standard architecture and conventions for Model Context Protocol (MCP) servers with streamable HTTP transport. This structure ensures consistency, maintainability, and production readiness across all MCP server implementations.

## Core Principles

1. **Modular Architecture** - Clear separation of concerns with dedicated modules
2. **Streamable HTTP First** - Modern HTTP transport as the primary interface
3. **Type Safety** - Full TypeScript coverage with proper interfaces
4. **Production Ready** - Built-in error handling, logging, and configuration management
5. **Testable** - Dependency injection and isolated components

## Directory Structure

```
project-root/
├── src/
│   ├── index.ts            # Main entry point
│   ├── cli.ts              # Command-line argument parsing
│   ├── config.ts           # Configuration management
│   ├── server.ts           # Server instance creation
│   ├── client.ts           # External API client
│   ├── types.ts            # TypeScript type definitions
│   ├── tools/
│   │   ├── index.ts        # Tool exports
│   │   └── [service].ts    # Tool definitions and handlers
│   └── transport/
│       ├── index.ts        # Transport exports
│       ├── http.ts         # HTTP transport (primary)
│       └── stdio.ts        # STDIO transport (development)
├── package.json
├── tsconfig.json
└── .gitignore
```

## Implementation Guide

### 1. Main Entry Point (`src/index.ts`)

The main entry point should handle transport selection and error management:

```typescript  theme={null}
#!/usr/bin/env node

import { config as loadEnv } from 'dotenv';
loadEnv();

import { loadConfig } from './config.js';
import { parseArgs } from './cli.js';
import { [Service]Server } from './server.js';
import { runStdioTransport, startHttpTransport } from './transport/index.js';

/**
 * Transport selection logic:
 * 1. --stdio flag forces STDIO transport
 * 2. Default: HTTP transport for production compatibility
 */
async function main() {
    try {
        const config = loadConfig();
        const cliOptions = parseArgs();
        
        if (cliOptions.stdio) {
            // STDIO transport for local development
            const server = new [Service]Server(config.apiKey);
            await runStdioTransport(server.getServer());
        } else {
            // HTTP transport for production/cloud deployment
            const port = cliOptions.port || config.port;
            startHttpTransport({ ...config, port });
        }
    } catch (error) {
        console.error("Fatal error running [Service] server:", error);
        process.exit(1);
    }
}

main();
```

### 2. Configuration Management (`src/config.ts`)

Centralized configuration with environment variable validation:

```typescript  theme={null}
import dotenv from 'dotenv';
dotenv.config();

export interface Config {
    apiKey: string;
    port: number;
    isProduction: boolean;
}

export function loadConfig(): Config {
    const apiKey = process.env['[SERVICE]_API_KEY'];
    if (!apiKey) {
        throw new Error('[SERVICE]_API_KEY environment variable is required');
    }

    const port = parseInt(process.env.PORT || '8080', 10);
    const isProduction = process.env.NODE_ENV === 'production';

    return { apiKey, port, isProduction };
}
```

### 3. Command Line Interface (`src/cli.ts`)

Standardized CLI with help documentation:

```typescript  theme={null}
export interface CliOptions {
    port?: number;
    stdio?: boolean;
}

export function parseArgs(): CliOptions {
    const args = process.argv.slice(2);
    const options: CliOptions = {};
    
    for (let i = 0; i < args.length; i++) {
        switch (args[i]) {
            case '--port':
                if (i + 1 < args.length) {
                    options.port = parseInt(args[i + 1], 10);
                    i++;
                } else {
                    throw new Error('--port flag requires a value');
                }
                break;
            case '--stdio':
                options.stdio = true;
                break;
            case '--help':
                printHelp();
                process.exit(0);
                break;
        }
    }
    return options;
}

function printHelp(): void {
    console.log(`
[Service] MCP Server

USAGE:
    [service] [OPTIONS]

OPTIONS:
    --port <PORT>    Run HTTP server on specified port (default: 8080)
    --stdio          Use STDIO transport instead of HTTP
    --help           Print this help message

ENVIRONMENT VARIABLES:
    [SERVICE]_API_KEY    Required: Your [Service] API key
    PORT                 HTTP server port (default: 8080)
    NODE_ENV            Set to 'production' for production mode
`);
}
```

### 4. Type Definitions (`src/types.ts`)

All TypeScript interfaces and types:

```typescript  theme={null}
/**
 * Arguments for [tool_name] tool
 */
export interface [Tool]Args {
    // Define tool-specific arguments
    query: string;
    options?: Record<string, unknown>;
}

/**
 * External API response structure
 */
export interface [Service]Response {
    // Define API response structure
    data: unknown;
    metadata?: Record<string, unknown>;
}
```

### 5. External API Client (`src/client.ts`)

Dedicated client for external API interactions:

```typescript  theme={null}
import { [Service]Response } from './types.js';

export class [Service]Client {
    private apiKey: string;
    private baseUrl: string = 'https://api.[service].com';

    constructor(apiKey: string) {
        this.apiKey = apiKey;
    }

    /**
     * Performs API request with proper error handling
     */
    async performRequest(params: unknown): Promise<string> {
        const response = await fetch(`${this.baseUrl}/endpoint`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${this.apiKey}`,
            },
            body: JSON.stringify(params),
        });

        if (!response.ok) {
            let errorText: string;
            try {
                errorText = await response.text();
            } catch {
                errorText = "Unable to parse error response";
            }
            throw new Error(
                `[Service] API error: ${response.status} ${response.statusText}\n${errorText}`
            );
        }

        const data: [Service]Response = await response.json();
        return this.formatResponse(data);
    }

    private formatResponse(data: [Service]Response): string {
        // Format response according to service requirements
        return JSON.stringify(data, null, 2);
    }
}
```

### 6. Tool Definitions (`src/tools/[service].ts`)

Tool definitions with handlers following the established pattern:

```typescript  theme={null}
import { Tool, CallToolResult } from '@modelcontextprotocol/sdk/types.js';
import { [Service]Client } from '../client.js';
import { [Tool]Args } from '../types.js';

/**
 * Tool definition for [tool_name]
 */
export const [tool]ToolDefinition: Tool = {
    name: "[service]_[action]",
    description: "Description of what this tool does and when to use it.",
    inputSchema: {
        type: "object",
        properties: {
            // Define input schema
        },
        required: ["required_field"],
    },
};

/**
 * Type guard for [tool] arguments
 */
function is[Tool]Args(args: unknown): args is [Tool]Args {
    return (
        typeof args === "object" &&
        args !== null &&
        "required_field" in args &&
        typeof (args as { required_field: unknown }).required_field === "string"
    );
}

/**
 * Handles [tool] tool calls
 */
export async function handle[Tool]Tool(
    client: [Service]Client, 
    args: unknown
): Promise<CallToolResult> {
    try {
        if (!args) {
            throw new Error("No arguments provided");
        }

        if (!is[Tool]Args(args)) {
            throw new Error("Invalid arguments for [service]_[action]");
        }

        const result = await client.performRequest(args);
        
        return {
            content: [{ type: "text", text: result }],
            isError: false,
        };
    } catch (error) {
        return {
            content: [
                {
                    type: "text",
                    text: `Error: ${error instanceof Error ? error.message : String(error)}`,
                },
            ],
            isError: true,
        };
    }
}
```

### 7. Tool Exports (`src/tools/index.ts`)

Centralized tool exports:

```typescript  theme={null}
export {
    [tool]ToolDefinition,
    handle[Tool]Tool,
    // Export all tool definitions and handlers
} from './[service].js';
```

### 8. Server Instance (`src/server.ts`)

Server configuration with tool registration:

```typescript  theme={null}
import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import {
    CallToolRequestSchema,
    ListToolsRequestSchema,
    InitializedNotificationSchema,
} from "@modelcontextprotocol/sdk/types.js";
import { [Service]Client } from './client.js';
import {
    [tool]ToolDefinition,
    handle[Tool]Tool,
} from './tools/index.js';

export function createStandaloneServer(apiKey: string): Server {
    const serverInstance = new Server(
        {
            name: "org/[service]",
            version: "0.2.0",
        },
        {
            capabilities: {
                tools: {},
            },
        }
    );

    const [service]Client = new [Service]Client(apiKey);

    serverInstance.setNotificationHandler(InitializedNotificationSchema, async () => {
        console.log('[Service] MCP client initialized');
    });

    serverInstance.setRequestHandler(ListToolsRequestSchema, async () => ({
        tools: [[tool]ToolDefinition],
    }));

    serverInstance.setRequestHandler(CallToolRequestSchema, async (request) => {
        const { name, arguments: args } = request.params;
        
        switch (name) {
            case "[service]_[action]":
                return await handle[Tool]Tool([service]Client, args);
            default:
                return {
                    content: [{ type: "text", text: `Unknown tool: ${name}` }],
                    isError: true,
                };
        }
    });

    return serverInstance;
}

export class [Service]Server {
    private apiKey: string;

    constructor(apiKey: string) {
        this.apiKey = apiKey;
    }

    getServer(): Server {
        return createStandaloneServer(this.apiKey);
    }
}
```

### 9. HTTP Transport (`src/transport/http.ts`)

Streamable HTTP transport with session management:

```typescript  theme={null}
import { createServer, IncomingMessage, ServerResponse } from 'http';
import { StreamableHTTPServerTransport } from '@modelcontextprotocol/sdk/server/streamableHttp.js';
import { SSEServerTransport } from '@modelcontextprotocol/sdk/server/sse.js';
import { randomUUID } from 'crypto';
import { createStandaloneServer } from '../server.js';
import { Config } from '../config.js';

const sessions = new Map<string, { transport: StreamableHTTPServerTransport; server: any }>();

export function startHttpTransport(config: Config): void {
    const httpServer = createServer();

    httpServer.on('request', async (req, res) => {
        const url = new URL(req.url!, `http://${req.headers.host}`);

        switch (url.pathname) {
            case '/mcp':
                await handleMcpRequest(req, res, config);
                break;
            case '/sse':
                await handleSSERequest(req, res, config);
                break;
            case '/health':
                handleHealthCheck(res);
                break;
            default:
                handleNotFound(res);
        }
    });

    const host = config.isProduction ? '0.0.0.0' : 'localhost';
    
    httpServer.listen(config.port, host, () => {
        logServerStart(config);
    });
}

async function handleMcpRequest(
    req: IncomingMessage,
    res: ServerResponse,
    config: Config
): Promise<void> {
    const sessionId = req.headers['mcp-session-id'] as string | undefined;

    if (sessionId) {
        const session = sessions.get(sessionId);
        if (!session) {
            res.statusCode = 404;
            res.end('Session not found');
            return;
        }
        return await session.transport.handleRequest(req, res);
    }

    if (req.method === 'POST') {
        await createNewSession(req, res, config);
        return;
    }

    res.statusCode = 400;
    res.end('Invalid request');
}

async function handleSSERequest(
    req: IncomingMessage,
    res: ServerResponse,
    config: Config
): Promise<void> {
    const serverInstance = createStandaloneServer(config.apiKey);
    const transport = new SSEServerTransport('/sse', res);
    
    try {
        await serverInstance.connect(transport);
        console.log('SSE connection established');
    } catch (error) {
        console.error('SSE connection error:', error);
        res.statusCode = 500;
        res.end('SSE connection failed');
    }
}

async function createNewSession(
    req: IncomingMessage,
    res: ServerResponse,
    config: Config
): Promise<void> {
    const serverInstance = createStandaloneServer(config.apiKey);
    const transport = new StreamableHTTPServerTransport({
        sessionIdGenerator: () => randomUUID(),
        onsessioninitialized: (sessionId) => {
            sessions.set(sessionId, { transport, server: serverInstance });
            console.log('New [Service] session created:', sessionId);
        }
    });

    transport.onclose = () => {
        if (transport.sessionId) {
            sessions.delete(transport.sessionId);
            console.log('[Service] session closed:', transport.sessionId);
        }
    };

    try {
        await serverInstance.connect(transport);
        await transport.handleRequest(req, res);
    } catch (error) {
        console.error('Streamable HTTP connection error:', error);
        res.statusCode = 500;
        res.end('Internal server error');
    }
}

function handleHealthCheck(res: ServerResponse): void {
    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ 
        status: 'healthy', 
        timestamp: new Date().toISOString(),
        service: '[service]-mcp',
        version: '0.2.0'
    }));
}

function handleNotFound(res: ServerResponse): void {
    res.writeHead(404, { 'Content-Type': 'text/plain' });
    res.end('Not Found');
}

function logServerStart(config: Config): void {
    const displayUrl = config.isProduction 
        ? `Port ${config.port}` 
        : `http://localhost:${config.port}`;
    
    console.log(`[Service] MCP Server listening on ${displayUrl}`);

    if (!config.isProduction) {
        console.log('Put this in your client config:');
        console.log(JSON.stringify({
            "mcpServers": {
                "[service]": {
                    "url": `http://localhost:${config.port}/mcp`
                }
            }
        }, null, 2));
        console.log('For backward compatibility, you can also use the /sse endpoint.');
    }
}
```

### 10. STDIO Transport (`src/transport/stdio.ts`)

Simple STDIO transport for development:

```typescript  theme={null}
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import { Server } from "@modelcontextprotocol/sdk/server/index.js";

export async function runStdioTransport(server: Server): Promise<void> {
    const transport = new StdioServerTransport();
    
    try {
        await server.connect(transport);
        console.error("[Service] MCP Server running on stdio");
    } catch (error) {
        console.error("Failed to start STDIO transport:", error);
        throw error;
    }
}
```

### 11. Transport Exports (`src/transport/index.ts`)

```typescript  theme={null}
export { startHttpTransport } from './http.js';
export { runStdioTransport } from './stdio.js';
```

## Configuration Files

### package.json Configuration

```json  theme={null}
{
  "name": "[service]-mcp-server",
  "version": "0.2.0",
  "type": "module",
  "main": "dist/index.js",
  "bin": {
    "[service]-mcp": "dist/index.js"
  },
  "files": [
    "dist"
  ],
  "scripts": {
    "build": "tsc && shx chmod +x dist/*.js",
    "prepare": "npm run build",
    "watch": "tsc --watch",
    "start": "node dist/index.js",
    "start:stdio": "node dist/index.js --stdio",
    "dev": "tsc && node dist/index.js",
    "dev:stdio": "tsc && node dist/index.js --stdio"
  },
  "dependencies": {
    "@modelcontextprotocol/sdk": "^1.17.3"
  }
}
```

**Key Configuration Points:**

* `"main": "dist/index.js"` - Points to compiled entry point
* `"bin"` - Makes the server executable as a CLI tool
* `"files": ["dist"]` - Only includes compiled code in npm package
* `"type": "module"` - Enables ES modules
* `"@modelcontextprotocol/sdk": "^1.17.3"` - **Required**: Version 1.16.0+ needed for StreamableHTTPServerTransport

### TypeScript Configuration

```json  theme={null}
{
  "compilerOptions": {
    "target": "ES2015",
    "module": "ESNext",
    "outDir": "./dist",
    "rootDir": ".",
    "strict": true,
    "esModuleInterop": true,
    "forceConsistentCasingInFileNames": true,
    "skipLibCheck": true,
    "moduleResolution": "node"
  },
  "include": [
    "src/**/*.ts"
  ],
  "exclude": [
    "node_modules",
    "dist"
  ]
}
```

**Build Process:**

1. TypeScript compiles `src/index.ts` → `dist/index.js`
2. Package.json points to compiled version
3. Build script makes output executable
4. No root-level index.ts needed

### .gitignore

```gitignore  theme={null}
# Build artifacts
node_modules/
dist/
npm-debug.log
yarn-error.log

# Environment files
.env
*.env

# IDE files
.vscode/
.idea/
*.swp
*.swo

# System files
.DS_Store
Thumbs.db
```

## Best Practices

### 1. Error Handling

* Always wrap API calls in try-catch blocks
* Provide meaningful error messages
* Log errors for debugging while sanitizing sensitive data

### 2. Type Safety

* Define interfaces for all data structures
* Use type guards for runtime validation
* Enable strict TypeScript checking

### 3. Session Management

* Implement proper session cleanup
* Handle connection timeouts
* Monitor memory usage for session storage

### 4. Production Readiness

* Use environment variables for configuration
* Implement health checks
* Add structured logging
* Consider rate limiting for external APIs

### 5. Testing

* Keep components isolated for easy unit testing
* Mock external API clients in tests
* Test both transport methods

## Migration Checklist

When refactoring an existing MCP server to this architecture:

* [ ] Create modular directory structure with `src/` folder
* [ ] Move main entry point to `src/index.ts` (single entry point)
* [ ] Extract configuration management (`src/config.ts`)
* [ ] Separate CLI argument parsing (`src/cli.ts`)
* [ ] Create dedicated API client class (`src/client.ts`)
* [ ] Define TypeScript interfaces (`src/types.ts`)
* [ ] Create server instance factory (`src/server.ts`)
* [ ] Move tool definitions to separate files (`src/tools/[service].ts`)
* [ ] Implement modular transport system (`src/transport/`)
* [ ] Add streamable HTTP transport as primary
* [ ] Configure package.json to point to `dist/index.js`
* [ ] Set up proper TypeScript compilation (`src/` → `dist/`)
* [ ] Add health check endpoint
* [ ] Update build scripts and .gitignore
* [ ] Add proper error handling throughout
* [ ] Test both HTTP and STDIO transport methods

This architecture ensures consistency, maintainability, and production readiness across all MCP server implementations while prioritizing the modern streamable HTTP transport.


